# Tensor2Tensor 从入门到放弃

[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or [T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library for deep learning models and datasets with [Tensorflow](https://www.tensorflow.org) as the backend. The ```T2T``` version that I used is 1.14.0 which is built on TensorFlow 1.14.0.

## ```T2T``` overview

### ```Problems```

```Problems``` consist of **features** such as inputs and targets, and **metadata** such as each feature's modality (e.g. symbol, image, audio) and **vocabularies**. In other words, a ```Problem``` is a dataset together with some fixed pre-processing. All ```Problems``` are registered with ```@registry.register_problem```, and any user-defined ```Problem``` should be registered first.

Some selected available problems are listed as follows.
* cola [[link](https://nyu-mll.github.io/CoLA/)]
* image_celeba [[link](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)]
* image_mnist
* sentiment_imdb
* translate_ende_wmt32k
* translate_enfr_wmt_multi64k

### ```Models```

```Models``` define the core **tensor-to-tensor** computation, which apply a default transformation to each input and output so that ```Models``` may deal with modality-independent tensors (e.g. embeddings at the input; and a linear transform at the output to produce logits for a softmax over classes). Similarly, all ```Models``` are registered with ```@registry.register_model``` and any user-defined ```Model``` should be registered first.

Some selected available models are listed as follows.
* resnet50
* transformer
* transformer_encoder
* shake_shake [[arXiv](https://arxiv.org/abs/1705.07485)]
* xception [[arXiv](https://arxiv.org/abs/1610.02357)]

### ```HParams```

```Hyperparameter Sets``` or ```HParams``` define a set of hyperparameters including num_hidden_layers, hidden_size, num_heads, attention_dropout, learning_rate, etc. Similarly, all ```HParams``` are registered with ```@registry.register_hparams``` and any user-defined ```HParams``` should be registered first.

### ```Trainer```

```Trainer``` is the entrypoint for training, evaluation, and inference. The additional command-line flags enable users to indicate the ```Problem```, ```Model``` and ```HParams```. As defined in ```bin/t2t_trainer.py```, some important command-line flags are listed as follows.
* ```t2t_usr_dir``` path to a Python module that will be imported (self-implemented problems, models, and hyperparameters)
* ```data_dir``` path to data directory (generated by ```t2t_datagen``` with ```TFRecord``` format)
* ```tmp_dir``` path to temporary storage directory (typically within the same dir as ```data_dir```)
* ```output_dir``` path to base output directory for run (including checkpoint, hparams.json, and eval)
* ```train_steps``` training steps (training epochs)
* ```eval_steps``` evaluation steps (every #epochs validation set is evaluated)

```t2t_datagen``` produces the training and development data for ```--problem``` into ```--data_dir```. Specifically, it produces sharded and shuffled ```TFRecord``` files of ```tensorflow.Example``` protocol buffers for a variety of registered datasets. The filenames for the generated data files would be similar to ```poetry_line_problem-train-00010-of-00090```, which indicates the number of batches into which training data is partitioned.

### Some demos
> running on TESLA P100-PCIE-16G

```
t2t-trainer \
    --generate_data \
    --data_dir=./t2t_data \
    --output_dir=./t2t_train/mnist \
    --problem=image_mnist \
    --model=shake_shake \
    --hparams_set=shake_shake_quick \
    --train_steps=1000 \
    --eval_steps=100
```
to run the shake-shake model (CIFAR) on MNIST dataset as classification task with pre-defined hyperparameters. The results of ```T2TModels```: loss=0.045, accuracy=0.995, accuracy_top5=1.0.

```
t2t-trainer \
    --generate_data \
    --data_dir=./t2t_data \
    --output_dir=./t2t_train/mnist \
    --problem=image_mnist \
    --model=resnet \
    --hparams_set=resnet_50 \
    --train_steps=1000 \
    --eval_steps=100
```
to replace the shake-shake model with 50-layer ResNet also on MNIST dataset as classification task. The results of ```T2TModels```: loss=0.576, accuracy=0.975, accuracy_top5=0.998.

```
t2t-trainer \
    --generate_data \
    --data_dir=./t2t_data \
    --output_dir=./t2t_train/sentiment \
    --problem=sentiment_imdb \
    --model=transformer \
    --hparams_set=transformer_tiny \
    --train_steps=1000 \
    --eval_steps=100
```
to run transformer model on IMDB movie reviews dataset as sentiment analysis task. The results of ```T2TModels```: loss=0.461, accuracy=0.772, accuracy_top5=1.0, and rouge_L_fscore=0.764 (sentence-level f1 score).


## Customization -- new ```Problem```

For each customized problem, we create a subclass of ```Problem``` (e.g. ```PoetryLines```) and register it so that it is accessible to command-line flag. Since many text-to-text problems share similar methods, a built-in class called ```Text2TextProblem``` that extends the base class ```Problem``` and makes it easy to add text-to-text problems. Note that the problem name is converted from ```CamelCase``` to ```snake_case``` in command-line flags.
```
from tensor2tensor.data_generators import problem
from tensor2tensor.data_generators import text_problems
from tensor2tensor.utils import registry

@registry.register_problem
class PoetryLines(text_problems.Text2TextProblem):
```

The text generated is encoded with a vocabulary for training. By default, it is a ```SubWordTextEncoder``` that is built with an approximate vocabulary size specified by the user. It is fully invertible (no OOV words) with a fixed-size vocabulary which makes it deal for text problems.
```
@property
def approx_vocab_size(self):
    return 2**13 // ~8k
```

When splitting data into training and evaluation sets, we need ```dataset_splits``` to determine the fraction of each split. Note that to no pre-existing "training" and "evaluation" sets, we set ```is_generate_per_split=False```.
```
@property
def dataset_splits(self):
return [{
    "split": problem.DatasetSplit.TRAIN,
    "shards": 9,
}, {
    "split": problem.DatasetSplit.EVAL,
    "shards": 1,
}]
```

Within ```problems``` (e.g. ```problems.problem('image_mnist')```), the ```generate_data``` is used to download data and/or process it into a standard format reading for training and evaluation. The arguments remain the same as ```data_dir```, ```tmp_dir``` and additional ```task_id```, seen as follows.
```
generate_data(data_dir, tmp_dir, task_id) 
        corresponding to problems.problem('image_mnist')
```


## Customization -- new ```Model```

```T2TModel``` has three typical usages:
1. **Estimator**: The method ```make_estimator_model_fn``` builds a ```model_fn``` for the tf.Estimator workflow of training, evaluation, and prediction. It performs the method ```call```, which performs the core *tensor-to-tensor* computation, followed by ```estimator_spec_train```, ```estimator_spec_eval```, or ```estimator_spec_predict``` depending on the tf.Estimator mode.

2. **Layer**: The method ```call``` enables ```T2TModel``` to be used as a callable by itself. It calls the following methods:
   * ```bottom```, which transforms features according to ```problem_hparams``` input and target ```Modality```;
   * ```body```, which takes features and performs the core model computation to return output and any auxiliary loss terms;
   * ```top```, which takes features and the body output, and transforms them according to ```problem_hparams``` input and target ```Modality``` to return the final logits;
   * ```loss```, which takes the logits, forms any missing training loss, and sum all loss terms.

3. **Inference**: The method ```infer``` enable ```T2TModel``` to make sequence predictions by itself.

The input features typically have ```"inputs"``` and ```"target"```, each of which is a **batched 4D Tensor**. For instance, the 4D Tensor is of shape ```[batch_size, sequence_length, 1, 1]``` for text input and of shape ```[batch_size, height, width, 3]``` for image input.

Here is an example class that extends ```T2TModel```
```
from tensor2tensor.utils import t2t_model
from tensor2tensor.utils import registry

@registry.register_model
class MyFC(t2t_model.T2TModel):
```
Have in mind that the name ```MyFC``` would become ```my_fc``` when used in command-line flags.

Furthermore, the model can certainly base on other models, like ```Transformer```. Here is another example class that extends ```Transformer```
```
from tensor2tensor.utils import t2t_model
from tensor2tensor.utils import registry
from tensor2tensor.model.transformer import Transformer

@registry.register_model
class Transformer_DelibNet(Transformer):
```


## Evaluation tricks

To address overfitting issues, we have several options. Firstly, we could visualize the training process with TensorBoard, which is intuitive. Secondly, we could apply early stopping criteria of ```approx_bleu``` metric on the validation set, which is time-saving. But beyond these approaches, we could also test our model on every pre-saved checkpoints. The underlying ideas are quite common, but here we need address it using Shell script. 
```

```


## Reference

* Tensor2Tensor github [[link](https://github.com/tensorflow/tensor2tensor)]
* Tensor2Tensor Intro on Colab [[link](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OPGni6fuvoTj)]